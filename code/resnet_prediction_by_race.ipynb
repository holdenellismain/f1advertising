{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "953e4127",
   "metadata": {},
   "source": [
    "This notebook does frame-by-frame classification for multiple races and saves the total counts to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af31fd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import mode\n",
    "from torchvision import models\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb1fbd6",
   "metadata": {},
   "source": [
    "Load in Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507e2aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformation that is compatible with resnet-18\n",
    "class CenterSquareCrop:\n",
    "    def __call__(self, img):\n",
    "        width, height = img.size\n",
    "        min_dim = min(width, height)\n",
    "        left = (width - min_dim) // 2\n",
    "        top = (height - min_dim) // 2\n",
    "        right = left + min_dim\n",
    "        bottom = top + min_dim\n",
    "        return img.crop((left, top, right, bottom))\n",
    "\n",
    "# Updated transform pipeline\n",
    "transform = transforms.Compose([\n",
    "    CenterSquareCrop(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet mean\n",
    "                         std=[0.229, 0.224, 0.225]),   # ImageNet std\n",
    "])\n",
    "\n",
    "# Reconstruct the same architecture\n",
    "def build_model(num_classes=5):\n",
    "    model = models.resnet18(pretrained=True)\n",
    "\n",
    "    # Freeze all layers\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Replace the final classification layer\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Linear(model.fc.in_features, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(256, num_classes)\n",
    "    )\n",
    "\n",
    "    # Unfreeze the classifier head\n",
    "    for param in model.fc.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    return model\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = build_model(num_classes=5).to(device)\n",
    "model.load_state_dict(torch.load(\"C:/Users/fires/Downloads/final_model_v2_weight.pth\", map_location=device))\n",
    "model.eval() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436f9ec9",
   "metadata": {},
   "source": [
    "### Sliding window with majority vote to smooth prediction noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2641ac64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mode_filter(arr, window_size):\n",
    "    results = np.empty_like(arr)\n",
    "    for i in range(window_size // 2, len(arr) - window_size // 2):\n",
    "            window = arr[i-window_size // 2:i+window_size // 2]\n",
    "            mode_result = mode(window, keepdims=True)\n",
    "            if mode_result.count[0] == 1:  # Tie\n",
    "                    results[i] = arr[i]  # Use original\n",
    "            else:\n",
    "                    results[i] = mode_result.mode[0]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25092da",
   "metadata": {},
   "source": [
    "Frame-by-frames from each race should be in a named folder within the \"path\" folder (in this case 2024Frames) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e568c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=[\"Track\",\"DistNoCar\", \"Front\", \"Inside\", \"Rear\", \"Side\"])\n",
    "\n",
    "path = \"C:/Users/fires/Downloads/2024Frames\"\n",
    "for folder in os.listdir(path):\n",
    "    folder_path = os.path.join(path, folder)\n",
    "    frame_paths = [folder_path + '/' + file for file in os.listdir(folder_path) if file.lower().endswith('.jpg')]\n",
    "    predictions = np.empty((0,), dtype=np.uint8)\n",
    "\n",
    "    for frame in frame_paths:\n",
    "        image = Image.open(frame).convert('RGB')\n",
    "        image = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(image)\n",
    "            _, predicted_class = torch.max(output, 1)\n",
    "            predicted_class_num = predicted_class.item()\n",
    "            predictions = np.append(predictions, predicted_class_num)\n",
    "\n",
    "    predictions = mode_filter(predictions, 7) # initial mode filter to clear up larger noise \n",
    "    predictions = mode_filter(predictions, 3) # mode filter to clear up individual frames that are misclassified\n",
    "\n",
    "    filtered = predictions[np.isin(predictions, [0, 1, 2, 3, 4])]\n",
    "    counts = np.bincount(filtered, minlength=5)\n",
    "    print(counts)\n",
    "    df.loc[len(df)] = [folder] + counts.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdddf03",
   "metadata": {},
   "source": [
    "Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84025621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Track  DistNoCar  Front  Inside  Rear  Side\n",
      "0         Abu Dhabi_frames       4733   2298    3191   565  1440\n",
      "1         Australia_frames       4566   2839    2346  1495  1109\n",
      "2          Austrian_frames       4263   2823    1922   976  2020\n",
      "3        Azerbaijan_frames       4153   2627    3217  1561   707\n",
      "4           Bahrain_frames       4211   2200    2260   898  1474\n",
      "5           Belgian_frames       5170   3241    1517   832  1459\n",
      "6           British_frames       6293   1516    1866   901  1571\n",
      "7            Canada_frames       5655   2891    1819   944   959\n",
      "8           Chinese_frames       4342   3003    2273  1207  1409\n",
      "9             Dutch_frames       5148   2035    2125  1210  1488\n",
      "10   Emilia Romanga_frames       3703   2759    3486  1155   900\n",
      "11        Hungarian_frames       4131   2486    1966  1589  2057\n",
      "12          Italian_frames       3748   3558    2814   670  1559\n",
      "13         Japanese_frames       4158   2813    2452   916  1706\n",
      "14        Las Vegas_frames       3830   4204    2247   904  1167\n",
      "15      Mexico City_frames       5588   2554    1991   686  1529\n",
      "16            Miami_frames       4495   2728    2266  1110  1584\n",
      "17           Monaco_frames       4156   2066    3626  1290  1048\n",
      "18            Qatar_frames       3145   2944    3210   736  2050\n",
      "19        Sao Paulo_frames       4978   2659    1846   985  1772\n",
      "20     Saudi Arabia_frames       3751   3373    2915   853  1266\n",
      "21        Signapore_frames       3394   2472    4097   801  1134\n",
      "22          Spanish_frames       3502   3257    2674   907  1660\n",
      "23    United States_frames       4917   2179    2312   902  2055\n"
     ]
    }
   ],
   "source": [
    "df.to_csv('angle_counts.csv')\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
