{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53b52bba",
   "metadata": {},
   "source": [
    "This notebook classifies an entire video frame-by-frame and allows the user to view the predictions over the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af31fd04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tensorflow/lib/python3.10/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/opt/anaconda3/envs/tensorflow/lib/python3.10/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib\n",
      "  Referenced from: <0B7EB158-53DC-3403-8A49-22178CAB4612> /opt/anaconda3/envs/tensorflow/lib/python3.10/site-packages/torchvision/image.so\n",
      "  Reason: tried: '/opt/anaconda3/envs/tensorflow/lib/python3.10/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/tensorflow/lib/python3.10/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/tensorflow/lib/python3.10/lib-dynload/../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/tensorflow/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import mode\n",
    "from torchvision import models\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb1fbd6",
   "metadata": {},
   "source": [
    "Load in Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507e2aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformation that is compatible with resnet-18\n",
    "class CenterSquareCrop:\n",
    "    def __call__(self, img):\n",
    "        width, height = img.size\n",
    "        min_dim = min(width, height)\n",
    "        left = (width - min_dim) // 2\n",
    "        top = (height - min_dim) // 2\n",
    "        right = left + min_dim\n",
    "        bottom = top + min_dim\n",
    "        return img.crop((left, top, right, bottom))\n",
    "\n",
    "# Updated transform pipeline\n",
    "transform = transforms.Compose([\n",
    "    CenterSquareCrop(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet mean\n",
    "                         std=[0.229, 0.224, 0.225]),   # ImageNet std\n",
    "])\n",
    "\n",
    "# Reconstruct the same architecture\n",
    "def build_model(num_classes=5):\n",
    "    model = models.resnet18(pretrained=True)\n",
    "\n",
    "    # Freeze all layers\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Replace the final classification layer\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Linear(model.fc.in_features, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(256, num_classes)\n",
    "    )\n",
    "\n",
    "    # Unfreeze the classifier head\n",
    "    for param in model.fc.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    return model\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = build_model(num_classes=5).to(device)\n",
    "model.load_state_dict(torch.load('final_model_v1_weight.pth', map_location=device))\n",
    "model.eval() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2339b6c4",
   "metadata": {},
   "source": [
    "Making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a285f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 3 ... 2 1 3]\n"
     ]
    }
   ],
   "source": [
    "path = \"C:/Users/fires/Downloads/russianGPframe-by-frame\" # path to folder with frame-by-frame images, downloaded with extract_frame.py\n",
    "\n",
    "frame_paths = [path + '/' + file for file in os.listdir(path) if file.lower().endswith('.jpg')]\n",
    "predictions = np.empty((0,), dtype=int)\n",
    "\n",
    "for frame in frame_paths:\n",
    "    image = Image.open(frame).convert('RGB')\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        _, predicted_class = torch.max(output, 1)\n",
    "        predicted_class_num = predicted_class.item()\n",
    "        predictions = np.append(predictions, predicted_class_num)\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436f9ec9",
   "metadata": {},
   "source": [
    "Sliding window with majority vote to smooth prediction noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2641ac64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mode_filter(arr, window_size):\n",
    "    results = np.empty_like(arr)\n",
    "    for i in range(window_size // 2, len(arr) - window_size // 2):\n",
    "            window = arr[i-window_size // 2:i+window_size // 2]\n",
    "            mode_result = mode(window, keepdims=True)\n",
    "            if mode_result.count[0] == 1:  # Tie\n",
    "                    results[i] = arr[i]  # Use original\n",
    "            else:\n",
    "                    results[i] = mode_result.mode[0]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e568c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = mode_filter(predictions, 7) # initial mode filter to clear up larger noise\n",
    "predictions = mode_filter(predictions, 3) # mode filter to clear up individual frames that are misclassified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa12037f",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {0: \"Distant or No Car\", 1: \"Front\", 2: \"Inside\", 3: \"Rear\", 4: \"Side\"}\n",
    "predictions_mapped = np.vectorize(lambda x: label_map.get(x, f'unknown: {x}'))(predictions)\n",
    "\n",
    "# Display settings\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "font_scale = 1\n",
    "color = (0, 255, 0)  # Green text\n",
    "thickness = 2\n",
    "position = (50, 50)  # Top-left corner\n",
    "\n",
    "# Loop through all frames\n",
    "for i in range(len(predictions_mapped)):\n",
    "    image = cv2.imread(frame_paths[i])\n",
    "    \n",
    "    if image is None:\n",
    "        print(f\"⚠️ Warning: Could not read {frame_paths[i]}\")\n",
    "        continue\n",
    "\n",
    "    label_text = f\"Label: {predictions_mapped[i]}\"\n",
    "    cv2.putText(image, label_text, position, font, font_scale, color, thickness, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow('Labeled Video', image)\n",
    "\n",
    "    key = cv2.waitKey(0)\n",
    "    if key == ord('q'):\n",
    "        cv2.destroyAllWindows()\n",
    "        cv2.waitKey(1)\n",
    "        break\n",
    "    elif key != ord('n'):\n",
    "        print(\"Press 'n' for next frame or 'q' to quit.\")\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
